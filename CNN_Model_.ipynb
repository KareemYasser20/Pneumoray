{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN Model .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOEUQCazG4nHT9kbOB9sNOo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KareemYasser20/Pneumoray/blob/main/CNN_Model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dul6clOsY-DX"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQmsmVLGTfMQ"
      },
      "source": [
        "# import os\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from matplotlib.image import imread\n",
        "# import pathlib\n",
        "\n",
        "# image_folder = ['BAC_PNEUMONIA', 'NORMAL', 'VIR_PNEUMONIA']\n",
        "# nimgs = {}\n",
        "# for i in image_folder:\n",
        "#     nimages = len(os.listdir('/content/drive/MyDrive/MixData/'+i+'/'))\n",
        "#     nimgs[i]=nimages\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# plt.bar(range(len(nimgs)), list(nimgs.values()), align='center')\n",
        "# plt.xticks(range(len(nimgs)), list(nimgs.keys()))\n",
        "# plt.title('Distribution of different classes of Dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coGgynR-WdyV"
      },
      "source": [
        "# # # Creating Train / Val / Test folders (One time use)\n",
        "# import os\n",
        "# import numpy as np\n",
        "# import shutil\n",
        "# import random\n",
        "# root_dir = '/content/drive/MyDrive/MixData/' # data root path\n",
        "# classes_dir = ['BAC_PNEUMONIA', 'NORMAL', 'VIR_PNEUMONIA'] #total labels\n",
        "\n",
        "# val_ratio = 0.05\n",
        "# test_ratio = 0.20\n",
        "\n",
        "\n",
        "\n",
        "# for cls in classes_dir:\n",
        "#   os.makedirs(root_dir +'train/' + cls)\n",
        "#   os.makedirs(root_dir +'val/' + cls)\n",
        "#   os.makedirs(root_dir +'test/' + cls)\n",
        "#   # Creating partitions of the data after shuffeling\n",
        "#   src = root_dir + cls # Folder to copy images from\n",
        "#   allFileNames = os.listdir(src)\n",
        "#   np.random.shuffle(allFileNames)\n",
        "#   train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
        "#                                                               [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), \n",
        "#                                                               int(len(allFileNames)* (1 - test_ratio))])\n",
        "#   train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
        "#   val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]\n",
        "#   test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\n",
        "\n",
        "#   print('Total images: ' + cls, len(allFileNames))\n",
        "#   print('Training: ' + cls, len(train_FileNames))\n",
        "#   print('Validation: '  + cls , len(val_FileNames))\n",
        "#   print('Testing: '  + cls, len(test_FileNames))\n",
        "\n",
        "#   # Copy-pasting images\n",
        "#   for name in train_FileNames:\n",
        "#       shutil.copy(name, root_dir +'train/' + cls)\n",
        "\n",
        "#   for name in val_FileNames:\n",
        "#       shutil.copy(name, root_dir +'val/' + cls)\n",
        "\n",
        "#   for name in test_FileNames:\n",
        "#       shutil.copy(name, root_dir +'test/' + cls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJogUJuWtAFa"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import SGD\n",
        "import glob\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization, Activation,AveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow. keras import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko4JIzuoc8Ju"
      },
      "source": [
        "train_dir = '/content/drive/MyDrive/MixData/train'\n",
        "test_dir = '/content/drive/MyDrive/MixData/test'\n",
        "val_dir = '/content/drive/MyDrive/MixData/val'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-XyUn86fWmH"
      },
      "source": [
        "def load_train(file_dir):\n",
        "  normal_case_dir = file_dir+'/NORMAL'\n",
        "  BAC_PNEUMONIA_case_dir = file_dir+'/BAC_PNEUMONIA'\n",
        "  VIR_PNEUMONIA_case_dir = file_dir+'/VIR_PNEUMONIA'\n",
        "  normal_casses =glob.glob(normal_case_dir+'/*.jpeg') \n",
        "  # normal_case_dir.glob('*.jpeg')\n",
        "  BacPenu_casses = glob.glob(BAC_PNEUMONIA_case_dir+'/*.jpeg')\n",
        "  # BacPenu_casses = BAC_PNEUMONIA_case_dir.glob('*.jpeg')\n",
        "  VirusPenu_casses = glob.glob(VIR_PNEUMONIA_case_dir+'/*.jpeg')\n",
        "  # VirusPenu_casses = VIR_PNEUMONIA_case_dir.glob('*.jpeg')\n",
        "  train_data = []\n",
        "  trian_Label = []\n",
        "  for img in normal_casses:\n",
        "    train_data.append(img)\n",
        "    trian_Label.append('NORMAL')\n",
        "  for img in BacPenu_casses:\n",
        "    train_data.append(img)\n",
        "    trian_Label.append('BAC_PNEUMONIA')\n",
        "  for img in VirusPenu_casses:\n",
        "    train_data.append(img)\n",
        "    trian_Label.append('VIR_PNEUMONIA')\n",
        "\n",
        "  df = pd.DataFrame(train_data)\n",
        "  df.columns=['images']\n",
        "  df['label']= trian_Label\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR2Kts-efyqN"
      },
      "source": [
        "train_data = load_train(train_dir)\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wou8AvUocvF"
      },
      "source": [
        "plt.bar(train_data['label'].value_counts().index , train_data['label'].value_counts().values)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGCToeNrqlY9"
      },
      "source": [
        "test_data = load_train(test_dir)\n",
        "test_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcTtp3apqMxG"
      },
      "source": [
        "\n",
        "plt.bar(test_data['label'].value_counts().index , test_data['label'].value_counts().values)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubCOPVdNrBVs"
      },
      "source": [
        "val_data = load_train(val_dir)\n",
        "val_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0lsVc8WrRi0"
      },
      "source": [
        "plt.bar(val_data['label'].value_counts().index , val_data['label'].value_counts().values)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bljEejI9xijt"
      },
      "source": [
        "# pre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y_uZnGixngF"
      },
      "source": [
        "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "     preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input, \n",
        "    #  preprocessing_function = tf.keras.applications.vgg16.preprocess_input,\n",
        "    # rotation_range=15,\n",
        "    # fill_mode=\"nearest\",\n",
        "\n",
        "     samplewise_center= True,\n",
        "     samplewise_std_normalization= True,\n",
        "    )\n",
        "\n",
        "\n",
        "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "     preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
        "        rescale=1./255,\n",
        "    #     rotation_range=15,\n",
        "    # fill_mode=\"nearest\",\n",
        "         samplewise_center= True,\n",
        "         samplewise_std_normalization= True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNvI_oy0xzhL"
      },
      "source": [
        " train_images = train_generator.flow_from_dataframe(\n",
        "        dataframe=train_data,\n",
        "        x_col='images',\n",
        "        y_col='label',\n",
        "        target_size=(224, 224),\n",
        "        class_mode='categorical',\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "    )\n",
        " \n",
        " test_images = test_generator.flow_from_dataframe(\n",
        "        dataframe=test_data,\n",
        "        x_col='images',\n",
        "        y_col='label',\n",
        "        target_size=(224, 224),\n",
        "        class_mode='categorical',\n",
        "        batch_size=8,\n",
        "        shuffle=False\n",
        "    )\n",
        " \n",
        " val_images = test_generator.flow_from_dataframe(\n",
        "        dataframe=val_data,\n",
        "        x_col='images',\n",
        "        y_col='label',\n",
        "        target_size=(224, 224),\n",
        "        class_mode='categorical',\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        \n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqtjoSpxyvwk"
      },
      "source": [
        "newmodel =Sequential()\n",
        "\n",
        "newmodel.add(Conv2D(16 , (3,3) , input_shape = (224,224,3), padding = 'same' , activation='relu' ))\n",
        "newmodel.add(MaxPooling2D( pool_size=(2, 2) , strides=(2, 2)))\n",
        "newmodel.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "newmodel.add(Conv2D(32 , (3,3), padding = 'same' , activation='relu' ))\n",
        "newmodel.add(MaxPooling2D( pool_size=(2, 2) , strides=(2, 2)))\n",
        "newmodel.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "newmodel.add(Conv2D(64 , (3,3), padding = 'same' , input_shape = (224,224,3) , activation='relu'))\n",
        "newmodel.add(MaxPooling2D( pool_size=(2, 2) , strides=(2, 2)))\n",
        "newmodel.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "newmodel.add(Conv2D(128 , (3,3), activation='relu' , padding = 'same'  ))\n",
        "newmodel.add(MaxPooling2D( pool_size=(2, 2) , strides=(2, 2)))\n",
        "newmodel.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "newmodel.add(Flatten())\n",
        "newmodel.add(Dense(512, activation='relu' ))\n",
        "newmodel.add(Dropout(0.3))\n",
        "\n",
        "newmodel.add(Dense(3 , activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQEBD_pry9sc"
      },
      "source": [
        "# callbacks = [es, mc]\n",
        "callbacks=[EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=4)],\n",
        "opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "newmodel.compile(optimizer=opt,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBq5D0XQzK4s"
      },
      "source": [
        "cnn = newmodel.fit(train_images , validation_data= val_images , epochs=100 , callbacks= callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd_lDSZ43ayk"
      },
      "source": [
        "Ev_test = newmodel.evaluate(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqUTjwzMbHbL"
      },
      "source": [
        "pd.DataFrame(cnn.history).plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJmfVf-c3Uvo"
      },
      "source": [
        "# plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1nkUjvRieZ_"
      },
      "source": [
        "\n",
        "# show the confusion matrix of our predictions\n",
        "\n",
        "# compute predictions\n",
        "predictions = newmodel.predict(test_images)\n",
        "y_pred = [np.argmax(probas) for probas in predictions]\n",
        "y_test = test_images.labels\n",
        "class_names = test_images.class_indices.keys()\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "# compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# plot normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3henp52ITj2z"
      },
      "source": [
        "cnf_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xn08Wduclyh"
      },
      "source": [
        "newmodel.save('/content/drive/MyDrive/MixData/lastcnn24_6.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyVpyPUKobGT"
      },
      "source": [
        "# saved_tt.evaluate(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e0vl-D2iHSu"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.plot(cnn.history['loss'], label='Training Loss')\n",
        "plt.plot(cnn.history['val_loss'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.ylabel('Accuracy', fontsize=16)\n",
        "plt.plot(cnn.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(cnn.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SYsxNQx3Z1L"
      },
      "source": [
        "accuracy = cnn.history['accuracy']\n",
        "val_accuracy = cnn.history['val_accuracy']\n",
        "loss = cnn.history['loss']\n",
        "val_loss = cnn.history['val_loss']\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7YHXkDJ4u1M"
      },
      "source": [
        "#Checking the calss indices: {'BAC_PNEUMONIA': 0, 'NORMAL': 1, 'VIR_PNEUMONIA': 2}\n",
        "proba_predictions= newmodel.predict(test_images)\n",
        "# print(proba_predictions)\n",
        "dict_classes= train_images.class_indices\n",
        "dict_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pazg0HfV4vW3"
      },
      "source": [
        "y_true= test_images.classes\n",
        "# y_predictions= (proba_predictions >0.6).astype('int32')\n",
        "print(classification_report(y_true,np.argmax(proba_predictions, axis=1) ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoQt9U6ZxjkV"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJftMHsiOUJk"
      },
      "source": [
        "# tf.keras.preprocessing.image.load_img(\n",
        "#     path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\"\n",
        "# )\n",
        "import numpy\n",
        "base_dir = '/content/drive/MyDrive/MixData/test/BAC_PNEUMONIA/'\n",
        "Virus_counter1 = 0\n",
        "Normal_counter1 = 0\n",
        "BAC_counter1 = 0\n",
        "cases1=0\n",
        "from keras.models import load_model\n",
        "# saved_model3 = load_model('/content/drive/MyDrive/MixData/newwwwwtest.h5')\n",
        "for f in sorted(os.listdir(base_dir)):\n",
        "  cases1 +=1\n",
        "  image = tf.keras.preprocessing.image.load_img(base_dir+f , target_size=(224,224 ,3))\n",
        "  input_arr = keras.preprocessing.image.img_to_array(image)\n",
        "  input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
        "  predictions = newmodel.predict(input_arr)\n",
        "  print (predictions)\n",
        "  pred_in = numpy.argmax(predictions, axis=1)\n",
        "  if pred_in==0: BAC_counter1 +=1\n",
        "  if pred_in==1: Normal_counter1 +=1\n",
        "  if pred_in==2: Virus_counter1 +=1\n",
        "  print (pred_in)\n",
        "\n",
        "print('MixData/test/VIR_PNEUMONIA file ')\n",
        "print('cases total = ' + str(cases1))\n",
        "print('BAC_counter = ' + str(BAC_counter1))\n",
        "print('Normal_counter = ' + str(Normal_counter1))\n",
        "print('Virus_counter = ' + str(Virus_counter1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}